[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Single Cell Analysis with R",
    "section": "",
    "text": "This is my attempt to leave the lab with some of my knowledge regarding single cell RNA-seq analysis. It is supposed to be user friendly, with some in-depth views when necessary, easy and reproducible code, and discussions on alternatives. It is in no way “the only way”, it may not be up to date after a while, and it will not cover all the bases. It should, however, be able to bring you to a decent analysis.\nMost of what I have learned, I did through:\n\ntrial and error\nthe OSCA book (sometimes referred to by me as “the bible”)\ncountless hours on Biostars, Stack Overflow, Bioconductor vignettes, manual pages, GitHub repositories, etc.\n\nAnd every day there is something new, no matter how small, that I can learn from any of these sources.\nSingle cell analysis is an incredibly fast field that is impossible to be up to date with. However, what matters is that we are able to have solid, interpretable results and not necessarily using the fanciest tools available. Having a good pipeline beats knowing how to use thousands of libraries by a long shot.\nThere is only one golden rule that you should always keep in mind when carrying out a complex analysis:\n\n\n\n\n\n\nYou must always keep in mind what kind of questions you are asking, whether the model is appropriate, and whether single cell sequencing can help you answer those questions.\n\n\n\n\n\n\nThere are many ways to analyze a dataset, and most of them will give you some sort of result. Whether or not that result makes sense does not only depend on the algorithms you use or the parameters you chose, but - for the most part - on what you expect to find and whether single cell sequencing is the right tool.\nHope this material is useful to you and your research!\nGiuseppe\nSingapore, August 2022"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "1  Summary",
    "section": "",
    "text": "Before you dive deep into single cell analysis it is useful to know what the whole process is about.\nYou have a bunch of cells, and in each cell you have quantified a bunch of genes. What can you do with these numbers? The most important thing is understand how each cell relates to each other transcriptome-wise. Are these cells all similar, or is there some heterogeneity? If there is heterogeneity, is it due to the fact that you sampled cells with different well-separated types or states - as in the case of a complex tissue such as a brain region - or is it due to these cells being captured along the continuum of a developmental process - as in the case of in vitro differentiation? If you are looking across different conditions such as healthy or diseased, are there any differences in cell type composition, or transcriptional programs within each cell type?\nTo answer these questions you will need to:\n\nmake sure your cells are of good quality, i.e. actual cells and “biologically healthy” (not stressed nor dying)\nmake sure cells can be compared to each other by removing technical differences\nselect the genes that change across all your cells according to biological variation, so as to avoid using all the genes (noisy and computationally expensive)\nembed cells in a lower dimensional space that is easier to work with and in which distances are a good proxy of how cells relate to each other (small distance = similar cells)\nuse the coordinates in this lower dimensional space and distances between cells to divide them in clusters, i.e. communities of cells that are similar to each other and different from the rest\nrelate these clusters to “actual” cell types, by looking at which genes (markers) they express and whether that expression is specific to those markers\nunderstand whether there are cell types whose transcriptional program changes across conditions such as healthy and diseased tissues\nunderstand whether the cell type composition of a tissue changes across conditions\n\nThen a standard single cell analysis pipeline is roughly composed of the following steps:\n\n\n\n\nflowchart TD\n  A(Input processing) --> B[QC]\n  B --> C[Normalization]\n  C --> D[Variable gene selection]\n  D --> E[Dimensionality reduction]\n  E --> F[Clustering]\n  F --> G[Cell type assignment]\n  G --> H[Differential expression]\n  G --> I[Differential composition]\n  \n\n\n\n\n\n\n\n\nThis is enough for you to answer most of the questions you or your collaborators may have: is there a new cell type that we did not observe in this tissue? is this gene expressed in this cell type? Is there a difference between healthy and diseased tissues?\nThis book will try to go through all these steps in practice considering common pitfalls and how to deal with them. The structure of the book will follow the pipeline so that it can be easy to take it up from a particular point in your analysis rather than going through all of it again.\nAt the beginning of each section I highlight the relevant software, i.e. the libraries or programs you should install to run the code. Every package is mentioned with the repository it is hosted on.\nThere are 3 ways to install R packages, depending on the repository:\n\n(R CRAN) packages can be installed by the base R command install(\"packagename\")\n(R Bioconductor) packages require that you first install the R CRAN package BiocManager, and then run BiocManager::install(\"packagename\").\n(R GitHub) packages require that you first install the R CRAN package devtools or remotes, and then run devtools::install_github(\"user/repository\") or remotes::install_github(\"user/repository\").\n\nFor more complex installations outside of R, the install commands are given explicitly and/or links to the appropriate resources are provided."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "datainput.html",
    "href": "datainput.html",
    "title": "3  Data input",
    "section": "",
    "text": "Relevant software:\n\n\n\nCellRanger (CLI Linux)\nSTAR (CLI Unix/Windows)\nkb-python (Python)\nSeurat (R CRAN)\nDropletUtils (R Bioconductor)\nSingleCellExperiment (R Bioconductor)"
  },
  {
    "objectID": "datainput.html#x-chromium",
    "href": "datainput.html#x-chromium",
    "title": "2  Data Input",
    "section": "2.2 10X Chromium",
    "text": "2.2 10X Chromium\nData created through a 10X Chromium platform will be available"
  },
  {
    "objectID": "datainput.html#obtaining-fastq-files",
    "href": "datainput.html#obtaining-fastq-files",
    "title": "3  Data input",
    "section": "3.2 Obtaining FASTQ files",
    "text": "3.2 Obtaining FASTQ files\nLet’s start by a common situation: you want to download some publicly available data to reanalyze it, and you want to start from the raw data.\n\n\n\n\n\n\nNote\n\n\n\nMost of the deposited data has already been processed by some standard quantification pipeline, making this step redundant most of the time. However, there are good reasons to start from the raw data again, e.g. if you are comparing datasets that have been processed by different pipelines, or if you need to use different transcript annotations.\n\n\nIdeally you would use the server, as it has access to a faster bandwidth and its the final destination of most of these files.\nThe server has the sratoolkit set of tools installed. This means that, if we want to download some external dataset that is deposited on the Sequence Read Archive (SRA), we only need to use prefetch.\nFirst, we need to know what samples we need. Let’s say we want to download the raw data (FASTQ files) from this publication by Schirmer et al. We go to the “Related information” section, and visit the link to SRA. At this point we want to look for the RUN accession number, which starts with “SRR” and will be the argument of prefetch. Note the argument -X 50G to prefetch which raises to 50GB the maximum download size. This may be necessary for large files.\n\nprefetch -X 50GB  SRR9123032\n\nThis dataset is comprised of 21 runs, so it is tedious to look for each of them manually. One solution is to go to the European Nucleotide Archive (ENA) website, look for the same run number in the search term box, and then select the whole project accession number (the link starting with “PRJNA”), leading to this page.\nFrom the project page we can select a report as a .TSV file, and use simple bash commands to isolate the column of interest removing the header:\n\ncut -f 4 filereport_read_run_PRJNA544731_tsv.txt | tail -n 21 > samples.txt\n\nThis file can be uploaded to the server in our working directory using scp or rsync.\nOnce we have the samples.txt file available, we run a loop using screen to download these files in parallel:\n\nfor i in `less samples.txt`; \ndo screen -dmS \"$i_prefetch\" `prefetch -X 50G $i`; \ndone\n\nThis will start a download in the background, which will create a new directory in your home named ncbi. The structure of the directory is:\n\nncbi/public/sra\n\nAnd the .sra files will be stored there.\nOnce the .sra files have been downloaded, we have to unpack them to get our FASTQ files. This is done by running fastq-dump on each .sra file, remembering to add the --split-files argument to separate paired end reads (or UMI-barcode reads) and the --gzip argument to compress the files.\n\ncd ~/ncbi/public/sra/\nfor i in *.sra; \ndo screen -dms \"$i_dump\" `fastq-dump $i --split-files --gzip`; \ndone\n\n\n\n\n\n\n\nWarning\n\n\n\nDo not delete the .sra files until you are sure that they are not corrupted."
  },
  {
    "objectID": "datainput.html#quantification",
    "href": "datainput.html#quantification",
    "title": "3  Data input",
    "section": "3.3 Quantification",
    "text": "3.3 Quantification\nYou have downloaded your FASTQ files, or your collaborator has sent them to you. Now you need to transform these raw data into transcript counts, so that you can import them into R and start with the analysis.\nThere are a few options available, depending on the platform you are working on, the type of raw data, and the resources at your disposal (in terms of memory and processing power).\n\n3.3.1 10X Chromium - CellRanger\nData created through a 10X Chromium platform will be available in a format that is already amenable to quantification through CellRanger. Unfortunately, CellRanger is only available for Linux. An installation of CellRanger is available on the server.\nThe file naming needs to follow a specific structure:\n\nFileName_SampleNumber_L001_R1_001.fastq.gz\nFileName_SampleNumber_L001_R2_001.fastq.gz\n\nwhere FileName is, for instance, the name of the run (SRR9123032), and SampleNumber is an internal convention (in this case, since this is the first sample, we can call it S1). R1 and R2 should be assigned to the barcode and the read+UMI respectively, but this in general is already taken care of by the submitter.\nFilenames after a run of fastq-dump will instead be:\n\nSRR9123032_1.fastq.gz\nSRR9123032_2.fastq.gz\n\nSo we need to rename them (using mv) to match CellRanger specifications.\nAssuming we are using the standard references that come packaged with CellRanger, we can run cellranger count specifying few parameters:\n\ncellranger count --id=S1 \\\n--transcriptome=/data/data/RefAnnot/cellRanger/refdata-cellranger-GRCh38-3.0.0 \\\n--fastqs=/data/data/SingleCell/giuseppe/schirmer/fastq/SRR9123032 \\\n--sample=SRR9123032 \\\n--chemistry=SC3Pv2 \\\n--localcores=10 \\\n--localmem=100 \\\n--expect-cells=4000 \\\n--include-introns\n\nCommenting the arguments:\n\n--id should match the SampleNumber portion of the file name we assigned earlier\n--transcriptome points to the reference (either distributed by CellRanger or custom)\n--fastqs points to where the FASTQ files are\n--sample indicates the sample name, matching the FileName we assigned earlier\n--chemistry=SC3Pv2 this depends on the publication, it is most likely either v2 or v3. However the default is auto which means CellRanger will detect it.\n--localcores=10 limit this to a reasonable number of cores\n--localmem=100 same but considering the amount of RAM you have at your disposal\n--expect-cells=4000 this is slightly above the default parameter, and it is informed by the publication\n--include-introns this is very important when dealing with single nuclei data, as most of the reads will come from pre-mRNA.\n\nDepending on the server load, 3 to 4 parallel runs (using screen) can be launched on the server.\nCellRanger will by default create .BAM (Binary Alignment Map) files. These are huge files that may not be useful as an end product. You can either include the --no-bam argument, or remove the BAM files after the run is finished.\n\n3.3.1.1 Output of CellRanger\nCellRanger outputs 3 important sets of files:\n\nan HTML report showing so-called “knee plots” (more on that later)\na folder of unfiltered quantifications\na folder of filtered quantifications\n\nFiltering works by applying an algorithm that detects empty droplets with ambient RNA and removes them. Normally, only the filtered quantifications are imported in R. However, if you import unfiltered quantifications, you need to apply empty droplet removal by yourself. I detail how in the QC chapter.\n\n\n\n3.3.2 STARsolo\nThis is an updated version of the popular alignment tool STAR, which has been tweaked to align UMI and barcode-based reads, but can be used to align full-length reads as well.\nI redirect you to the STARsolo github page for some indications on how to use it."
  },
  {
    "objectID": "datainput.html#kallisto-bustools",
    "href": "datainput.html#kallisto-bustools",
    "title": "3  Data input",
    "section": "3.4 kallisto | bustools",
    "text": "3.4 kallisto | bustools\nThis is a completely different workflow, which makes use of tools built by Lior Pachter’s group: the kallisto pseudoaligner and bustools format specification for single cell data.\nIn a nutshell, rather than downloading FASTQ files first and then using memory-intensive reference-based alignment, this pipeline “streams” reads directly from a repository through the quantification pipeline, saving space, memory, and time.\nkallisto and bustools are available as a single Python package which can be installed via pip as follows:\n\npip install kb-python\n## if using python >= 3\npip3 install kb-python\n\nOr, if you are working on the server and using your own conda environment:\n\nconda install -c bioconda kb-python\n\nThen, we can follow the instructions at this tutorial, which is a little bit out of date. I provide an updated version that should hopefully work for everyone.\nFirst we want to download the necessary references for the kallisto pseudoaligner. These are the transcript indices (necessary for pseudo-alignment of reads) and a table mapping ENSEMBL transcript IDs to gene IDs and symbols. Following the example in the tutorial, we download indices and transcript references for Mus musculus.\n\n## Download the transcript indices \ncurl -L https://github.com/BUStools/getting_started/releases/download/getting_started/Mus_musculus.GRCm38.cdna.all.idx.gz -o Mus_musculus.GRCm38.cdna.all.idx.gz\n\n## Download transcript to genes\ncurl -L https://github.com/BUStools/getting_started/releases/download/getting_started/transcripts_to_genes.txt -o transcripts_to_genes.txt\n\n## Extract references\ngunzip Mus_musculus.GRCm38.cdna.all.idx.gz\n\n## Assign variables\nREFPATH=$PWD/Mus_musculus.GRCm38.cdna.all.idx\nTXPATH=$PWD/transcripts_to_genes.txt\n\nThen, we can stream two FASTQ files from the Short Read Archive (SRA) from Koren et al. 2019.\n\nmkdir mouse_data\ncd mouse_data\n\n## Make named pipes (outputs)\nmkfifo R1.gz R2.gz \n\ncurl -Ls https://github.com/bustools/getting_started/releases/download/getting_started/SRR8599150_S1_L001_R1_001.fastq.gz  > $PWD/R1.gz &\ncurl -Ls https://github.com/bustools/getting_started/releases/download/getting_started/SRR8599150_S1_L001_R2_001.fastq.gz  > $PWD/R2.gz &\nkb count -i $REFPATH -x 10xv2 -g $TXPATH -t 4 -o $PWD --cellranger $PWD/R1.gz $PWD/R2.gz\n\nIn the code above, when calling kb-count we specify a few parameters:\n\n-i the path to the index, which we assigned to a variable $REFPATH\n-x the technology, in our case 10xv2\n-g the path to the transcript specification, assigned to $TXPATH\n-t the number of threads - change according to your machine\n-o the output directory, which is the present working directory $PWD\n--cellranger an option to create a CellRanger-compatible output\ntwo positional arguments specifying the two named pipes (which contain the FASTQ files) created at the beginning.\n\n\n\n\n\n\n\nWarning\n\n\n\nUnlike CellRanger and STARsolo, kb count does not filter empty droplets. If you use this workflow, removing empty droplets is a part of processing that should be taken care of in R.\n\n\nThis process is quite fast and has a light footprint, meaning that with a few loops (or jobs, if you are using a High Performance Computing Cluster) should take care of large amounts of data in little time. The real advantage of using this workflow is that you can process raw data on a laptop without the need to have access to high performance machines."
  },
  {
    "objectID": "datainput.html#importing-into-r",
    "href": "datainput.html#importing-into-r",
    "title": "3  Data input",
    "section": "3.5 Importing into R",
    "text": "3.5 Importing into R\nNow that the processing is done, we want to import the counts into R.\nThe two most popular packages for single cell transcriptomics analysis in R are Seurat and the SingleCellExperiment (Bioconductor) tools.\nI discuss their differences in the pipeline choice chapter, but for now I will show how to import CellRanger outputs (which can be created by the kb count pipeline as well) as either Seurat or SingleCellExperiment objects. Note that other outputs (such as STARsolo) can be imported just as easily.\n\n3.5.1 Importing in Seurat\nSeurat can be installed from CRAN as follows:\n\ninstall.packages(\"Seurat\")\nlibrary(Seurat)\n\nThen, we can directly read the data as a sparse matrix (a memory-efficient representation of matrices where most of the entries are 0’s). Assuming the CellRanger output is in the ./cellranger path:\n\ncounts <- Read10X(data.dir = \"./cellranger\")\nseurat_object = CreateSeuratObject(counts = counts)\n\n\n\n3.5.2 Importing in SingleCellExperiment\nFor this import we need the DropletUtils package from Bioconductor:\n\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"DropletUtils\")\nlibrary(DropletUtils)\n\nThen, the creation of a SingleCellExperiment object only requires one line:\n\nsce <- read10xCounts(samples = \"./cellranger\")"
  },
  {
    "objectID": "datainput.html#understanding-the-input",
    "href": "datainput.html#understanding-the-input",
    "title": "3  Data input",
    "section": "3.1 Understanding the input",
    "text": "3.1 Understanding the input\nSingle-cell RNA-seq raw data come (mostly) in two flavours:\n\n3’ UMI (Unique Molecular Identifiers) - and more rarely 5’ UMI\nFull length (can be UMI or not)\n\nThe most important difference is in the throughput. UMI 3’ counts are compatible with droplet-based technologies, such as 10X Chromium, DropSeq and others. Full-length is compatible with plate-based (sorting) technologies such as SmartSeq, or microfluidic technologies such as Fluidigm C1.\n\n\n\n\n\n\nNote\n\n\n\nIt is entirely possible to use UMI library preparation with a plate-based platform; UMIs were introduced to automatically de-duplicate reads and afford high precision. However, full length sequencing allows to capture more facets of gene expression such as alternative splicing or allele-specific expression.\n\n\nPlate-based platforms are limited by the number of plates; each plate can hold at most up to 384 wells, meaning 384 single cells. Fluidigm chips can hold each up to 800 single cells.\nDroplet-based technologies such as 10X Genomics Single Cell profiling (which combines droplets with microfluidic chips), up to 80,000 cells (320,000 with multiplexing in the High Throughput version) can be assayed in parallel.\nMany projects generate 10X data, which uses a UMI 3’ library generation procedure.\nThe downside of using droplet-based platforms is that there is a trade-off between throughput (number of cells) and depth (number of reads) per cell. SmartSeq/C1 datasets will have fewer cells but a higher number of reads quantified, and therefore a higher number of genes detected.\nWhen looking at UMI-based counts, you will have 2 files:\n\nRead 1 (R1) contains the UMI + sample barcode, i.e. a sequence that is unique to the droplet (ideally the cell) and unique to the transcript\nRead 2 (R2) contains the transcript read, i.e. a sequence that is unique to the transcript.\n\nThe combination of both reads gives the quantification of expression of a transcript within a droplet. The phenomenon of “empty droplets” happens when, for a given barcode, all of its R1 contains the barcode, and R2 contains the index, with no transcript read. This means that the droplet was sequenced and amplified but it did not contain any RNA.\n\n\n\n\n\n\nNote\n\n\n\nAs far as I can tell by looking at unfiltered data, an extremely high number of reads goes into sequencing empty droplets."
  },
  {
    "objectID": "qc.html",
    "href": "qc.html",
    "title": "4  Quality Control",
    "section": "",
    "text": "Relevant software:\n\n\n\nDropletUtils (R Bioconductor)\nscater (R Bioconductor)\nscuttle (R Bioconductor)\nscran (R Bioconductor)\nscDblFinder (R Bioconductor)\nscRNAseq (R Bioconductor)\nSingleCellExperiment (R Bioconductor)\nOnce an object has been created in R, you have to make sure that you will be working with good data."
  },
  {
    "objectID": "pipelinechoice.html",
    "href": "pipelinechoice.html",
    "title": "2  Choice of pipeline",
    "section": "",
    "text": "Before we delve into the details of a pipeline, it can be useful to look at the differences between the two most common (but not the only) single cell analysis toolkits: Seurat and the SingleCellExperiment tools.\nThis is not a benchmarking study, but rather a short writeup that reflects my experience using these tools and some of the advantages and disadvantages that I have personally encountered.\n\n\nSeurat exists as a stand-alone library, created and maintained by the group of Rahul Satija at the New York Genome Center. It contains a multitude of routines and functions to from the import of raw files to the final analysis of several types of datasets.\nIt is, by all accounts, a one-stop solution to perform single cell RNA, spatial transcriptomics and multi-modal -omics analyses (such as integration with feature barcoding or ATAC-seq).\nSeurat is rather dependency-heavy, with some system dependencies (software that must be installed outside of R) that can make some installs a bit complicated. It heavily relies on Python libraries for several applications, which are called through the reticulate library.\nThe Seurat developers write extremely easy to follow documentation, with tutorials that make single cell analysis accessible to anyone with minimal coding experience. A dataset could be analyzed quite quickly with few lines of code, if you trust what that code is doing. Efficiency is indeed one of the attractive features of Seurat, although in many cases it has been out-performed by some other packages.\nThe issue with being a one-stop solution is that many choices are made by the developers before they can be made by the users. The use of modularity clustering algorithms - popularized by Seurat itself - is basically the only choice; theoretical discussions around the choice of parameters in the documentation are not many. In other words, Seurat takes away some of the complexity by also reducing some of the freedom and blackboxing many of its functions.\nFor a more technical - and slightly biased, probably - critique of Seurat you can read this answer by Aaron Lun on GitHub.\n\n\n\nWith SingleCellExperiment toolkit I identify a family of inter-related packages on Bioconductor that were developed to analyze all sorts of single cell -omics data. There is no single library that can do everything, but it all revolves around the SingleCellExperiment class object, which is derived by the older SummarizedExperiment class.\nPackages that operate on and around this class usually take care of single aspects of the analysis.\nFor instance, scater implements quality control and visualizations; bluster contains a collection of clustering algorithms; scran contains functions for normalization, variance modelling, and differential expression; batchelor has functions to perform batch effect correction/integration; AUCell assigns cell identities; slingshot and TSCAN calculate pseudotime trajectories; etc.\nThe idea behind the Bioconductor philosophy is that the user is tasked with building their own pipeline using a highly modular collection of tools optimized for interoperability. This means that any developer can add their own package to the family and, depending on how well it works (on its own but especially with the other packages), it can become part of a more or less standardized pipeline.\nMany developers working on many packages in parallel means that there is a high diversity of approaches, but also some fragmentation. Some Bioconductor packages may be abandoned by their developers, or may be extremely inefficient in their use of memory and/or CPU time.\nIn general, Bioconductor packages are very well maintained and highly efficient and interoperable. Some contributions to Bioncoductor may be less efficient - for instance, not all packages have implemented parallelization or out-of-memory operations - but they still have the benefit of working with the same objects and within the same set of rules.\nThe documentation available for Bioconductor is, most of the time, excellent; the OSCA book (from which this book heavily borrows for inspiration) is the most complete single source of information for single cell analyses as it contains not only case studies but also discussions on theoretical and practical aspects of the analysis at an appropriate level of depth.\nSometimes, however, Bioconductor packages lag a little behind innovations in Seurat. Spatial analysis was more developed in Seurat than in any other Bioconductor package; similarly, integration with ATAC-seq and other modalities was implemented earlier in Seurat (although it could still be achieved by using the right combination of Bioconductor packages).\n\n\n\nMany analysis tools in R try to cater to both crowds, implementing functions that can automatically recognize either Seurat or SingleCellExperiment objects. Moreover, both approaches try to maintain some level of interoperability between them (e.g. functions in Seurat to convert to SingleCellExperiment objects) and, to a certain extent, similar functions.\nTo summarize, here is a small table highlighting the strengths of both approaches.\n\n\n\n\n\n\n\nSeurat\nSingleCellExperiment\n\n\n\n\nUser friendliness\nHighly customizable pipelines\n\n\nGood documentation (tutorials)\nGreat documentation (OSCA)\n\n\nEfficiency\nEfficiency (some)\n\n\nOne-stop solution\nModularity and interoperability\n\n\nWide array of formats (spatial, multi-modal)\nMany applications for transcriptomics not covered by Seurat (pseudotime, velocity, GRN reconstruction, etc)\n\n\nSingle developer - cohesive product\nDiversity - many opportunities but sometimes patchy\n\n\n\nI personally prefer the SingleCellExperiment family for its modularity, clarity of code and diversity, but I often use Seurat for some functions and approaches that may in some cases be faster, more efficient or give a better result.\nMoreover, for Python users out there there is a whole other universe revolving around scanpy and scVI, collectively termed scVerse.\nThroughout this book I will mostly focus on SingleCellExperiment solutions and, where convenient or interesting, point out the Seurat equivalent."
  },
  {
    "objectID": "pipelinechoice.html#singlecellexperiment",
    "href": "pipelinechoice.html#singlecellexperiment",
    "title": "2  Choice of pipeline",
    "section": "2.2 SingleCellExperiment",
    "text": "2.2 SingleCellExperiment\nWith SingleCellExperiment toolkit I identify a family of inter-related packages on Bioconductor that were developed to analyze all sorts of single cell -omics data. There is no single library that can do everything, but it all revolves around the SingleCellExperiment class object, which is derived by the older SummarizedExperiment class.\nPackages that operate on and around this class usually take care of single aspects of the analysis.\nFor instance, scater implements quality control and visualizations; bluster contains a collection of clustering algorithms; scran contains functions for normalization, variance modelling, and differential expression; batchelor has functions to perform batch effect correction/integration; AUCell assigns cell identities; slingshot and TSCAN calculate pseudotime trajectories; etc.\nThe idea behind the Bioconductor philosophy is that the user is tasked with building their own pipeline using a highly modular collection of tools optimized for interoperability. This means that any developer can add their own package to the family and, depending on how well it works (on its own but especially with the other packages), it can become part of a more or less standardized pipeline.\nMany developers working on many packages in parallel means that there is a high diversity of approaches, but also some fragmentation. Some Bioconductor packages may be abandoned by their developers, or may be extremely inefficient in their use of memory and/or CPU time.\nIn general, packages created and/or maintained by Aaron Lun - one of the driving forces behind most of the Bioconductor single cell universe - are very well maintained and highly efficient and interoperable. Other contributions to Bioncoductor may be less efficient but they still have the benefit of working with the same objects and within the same set of rules.\nThe documentation available for Bioconductor is, most of the time, excellent; the OSCA book (from which this book heavily borrows for inspiration) is the most complete single source of information for single cell analyses as it contains not only case studies but also discussions on theoretical and practical aspects of the analysis at an appropriate level of depth.\nSometimes, however, Bioconductor packages lag a little behind innovations in Seurat. Spatial analysis was more developed in Seurat than in any other Bioconductor package; similarly, integration with ATAC-seq and other modalities was implemented earlier in Seurat (although it could still be achieved by using the right combination of Bioconductor packages).\nTo summarize, here is a small table highlighting the strengths of both approaches.\n\n\n\n\n\n\n\nSeurat\nSingleCellExperiment\n\n\n\n\nInnovation\nDiversity\n\n\nGood documentation (tutorials)\nGreat documentation (OSCA)\n\n\nEfficiency\nEfficiency (some)\n\n\nOne-stop solution\nModularity and interoperability\n\n\nWide array of formats (spatial, multi-modal)\nMany applications not covered by Seurat (pseudotime, velocity, GRN reconstruction, etc)\n\n\n\nI personally prefer the SingleCellExperiment family, but I often look at Seurat for some functions and approaches that may be faster, more efficient or give a better result."
  },
  {
    "objectID": "datainput.html#adding-metadata",
    "href": "datainput.html#adding-metadata",
    "title": "3  Data input",
    "section": "3.6 Adding metadata",
    "text": "3.6 Adding metadata\nIf you have other information available regarding the samples - patient of origin, RNA Integrity Number, date of collection, assigned cell type, experimental condition, etc - you should include it in the colData slot of the SingleCellExperiment object, or the metadata slot of the Seurat object. Metadata like this is usually reported at the cell-level, meaning that there has to be a correspondence between the columns of the assay and the rows of the metadata. Both approaches will fail if you try to create a metadata/colData whose rows don’t match with the columns of the counts table.\nAssuming this cell-level metadata is in a tab-separated file named “cell_data.txt”, for the SingleCellExperiment previously generated you can add it as follows:\n\ncoldata <- read.delim(\"cell_data.txt\", sep = \"\\t\")\ncolData(sce) <- coldata\n\nFunctions such as colData, rowData, assay, reducedDim are both S4 getters and setters, meaning they can either return the content of a slot (e.g. pca <- reducedDim(sce, name = \"pca\") or overwrite it (e.g. reducedDim(sce, name = \"pca2\") <- pca2).\nIn Seurat, getters and setters are separated. Weirdly enough the metadata getter is [[]].\n\nseurat_object = AddMetaData(seurat_object, metadata = coldata)\nseurat_metadata = seurat_object[[]]"
  },
  {
    "objectID": "datainput.html#final-note-on-objects",
    "href": "datainput.html#final-note-on-objects",
    "title": "3  Data input",
    "section": "3.8 Final note on objects",
    "text": "3.8 Final note on objects\nOk, we got our input and we sort of understand what it looks like. But why do we need to import it into a Seurat or SingleCellExperiment object in the first place if they are just counts and metadata?\nThese objects have slots that hold different types of metadata or additional assays. For instance, in SingleCellExperiment objects:\n\nassays, i.e. tables containing feature-level quantification where rows are features (genes) and columns are cells. assays may contain the raw counts, log-normalized logcounts, batch-corrected corrected assays, and any other assay the user may want to add.\ncolData: a table containing cell-level information, such as QC metrics (number of transcripts detected, sum of reads, % of mitochondrial reads, batch of origin, assigned cell type, etc)\nrowData: a table containing feature-level information, such as alternative symbols, statistics, etc\nmetadata: a slot for additional data that do not follow either a gene- or cell-major order\nreducedDim: a slot for dimensionality reductions such as pca, umap, tsne, corrected\n\nand other slots.\nHaving everything in one object is not only “neater” but also allows to perform some operations easily: subsetting a SingleCellExperiment object by row - like you would with a matrix or data.frame - will subset all assays and rowData, as you are effectively subsetting over features; similarly, subsetting by column will subset assays, reducedDim and colData over cells.\nCalling a named element from the list - e.g. sce$patient will subset from the colData directly.\nMoreover, almost all functions from the Bioconductor family are written with the SingleCellExperiment will dispatch specific methods to this class. In other words, functions will automatically recognize a SingleCellExperiment object and perform certain operations that they wouldn’t do when called upon a matrix or a list. So, for instance, runPCA can be run on a matrix, thus returning an object containing the rotations, embeddings, and standard deviation calculations, or it can be run on a SingleCellExperiment object automatically using specific slots and populating the relevant reducedDim slot.\nMany functions in fact take the whole object as an input, and return the same object with additional data in the relevant slots.\nSeurat objects have a very similar grammar, they can be subset in a nearly identical way and have their own methods dispatch."
  },
  {
    "objectID": "qc.html#things-to-watch-out-for",
    "href": "qc.html#things-to-watch-out-for",
    "title": "4  Quality Control",
    "section": "4.1 Things to watch out for",
    "text": "4.1 Things to watch out for\nIn the previous chapter we saw that a lot of sequencing reads will be assigned to empty droplets. Removing those is just the beginning. Here is an incomplete list of things that you should assess critically:\n\nbalanced experimental design\nThis is not something that you can fix post-sequencing, but you can help your collaborator with at the time of planning. Experiments should be well designed, meaning that the size, timing and grouping of experimental samples avoids the introduction of uninteresting variation. Controls should be processed along with cases, in the same round; they can be split in different rounds only if for every control there is a corresponding case. Processing of samples should not be so long that the time difference between the first and the last sample consists of several hours. Sex, age, strain/ethnicity/background, should all be balanced (unless they are the subject of the study).\nambient RNA contamination (aka “the soup”).\nCells may have been lysed in the culture medium/dissociation solution before they were captured in droplets/wells, releasing the content of their cytoplasms. This now cell-free RNA can still be included in droplets/wells, captured, amplified, and sequenced. Ambient RNA contamination can be a big issue for differential expression studies.\nhigh relative mitochondrial gene expression\nA cell that is expressing a high level of mitochondrial RNA is a cell that was already undergoing apoptosis or other forms of cell death, as the mitochondrial transcripts would have been released in the cytoplasm. It is a common conception in the field that, by discarding cells with a high % of mitochondrial transcripts, you are actually discarding suffering/dying cells that would confuse the analysis. However, some cell types (such as myocytes) can be rich in mitochondria owing to their big energy requirements. When\nlow/high detection of transcripts\nCells with too few reads or too many reads are usually discarded as damaged/ambient and doublets, respectively. Small cells, however, may have fewer RNA molecules.\neffect of the cell cycle\nCell populations that are actively cycling may present different transcriptional profiles according to the phase of the cell cycle that they are in. If this effect is not accounted for, you could be mislead to think that there are 3 different cell populations (one for every detected phase); in reality the population is one, but it is cycling in an unsynchronized fashion. Moreover, when analyzing single nucleus RNA-seq, the presence of transcripts that are exclusively cytoplasmic or mitochondrial may point to poor library preparation, as the cytoplasm has not been stripped from the fixed tissues well enough, possibly creating confounding effects.\nHigh MALAT1 expression\nIn humans MALAT1 is transcribed into a long non-coding RNA that is systematically and reliably quantified by most poly-A capture-based sequencing methods, effectively soaking up many sequencing reads. As a highly expressed gene, it may also appear as highly variable, thus creating the illusion that it is a biologically relevant marker. Evidence suggests that MALAT1 is another stress-associated marker rather than an actual biological one. However, there are some cases (e.g. some cancers and leukaemias) in which MALAT1 may play a more important function.\nnumber of cells\nVery trivially but importantly, the effective number of cells is important to be able infer properties from a biological system. Samples which have too few cells will fail to recapitulate the cell type composition of a tissue, and will most likely be over-clustered or overfitted by most algorithms. Collections of samples with highly unequal numbers of cells require additional steps to be included to account for these differences.\nlow/absent gene quantification\nChances are most of the genes will not be detected at all in your dataset. This does not mean that they were not expressed, but only that they were not measured due to low input and competition for sequencing reagent from more abundant transcripts. The absence of evidence is not the evidence of absence, and I will discuss a little more the problem of dropouts later on. Additionally, datasets in which only few genes are quantified and with a lot of noise may not be reliable.\ndoublets\nAny single cell isolation technique will present the risk of incorporating more than one single cell per library preparation, meaning the same barcode will be shared by two different cells. These are called doublets and, if not accounted for, may look like intermediate cell states or entirely new cell populations.\nbatch effects\nDatasets coming from different individuals, different captures and/or different technologies will inevitably present batch effects. These are systematic biases in gene expression that we are not interested in, but nevertheless affect the analysis and interpretation. I will discuss batch effects later on, but they should already be tackled at the QC stage, since they can skew the distribution of metrics and the decision over which cells to keep and which to discard.\n\nThere are other issues that can be more specific to certain technologies or tissues. For instance, the dissociation protocol may deplete a cell type, or it may activate an expression signature in a cell type (e.g. immune responses) that may look like a phenotype, etc."
  },
  {
    "objectID": "qc.html#basic-initial-qc-plots",
    "href": "qc.html#basic-initial-qc-plots",
    "title": "4  Quality Control",
    "section": "4.2 Basic initial QC plots",
    "text": "4.2 Basic initial QC plots\nLet’s use our sce object created through the kallisto|bustools pipeline in Section 3 (data input).\n\n\n\nAs you will remember, it is an unfiltered dataset coming from a single mouse sample.\n\n4.2.1 Total counts and detected genes\nWe start by applying per-cell quality control using the scuttle package:\n\nlibrary(scuttle)\nlibrary(scater)\n\nsce = addPerCellQC(sce)\n\nWe can then visualize the metrics using violin plots:\n\nplotColData(object = sce, y = \"sum\") + scale_y_log10()\n\nWarning: Transformation introduced infinite values in continuous y-axis\nTransformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 7908 rows containing non-finite values (stat_ydensity).\n\n\nWarning: Removed 7908 rows containing missing values (geom_point).\n\n\n\n\nplotColData(object = sce, y = \"detected\") + scale_y_log10()\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 7908 rows containing non-finite values (stat_ydensity).\n\n\nWarning: Removed 7908 rows containing missing values (geom_point).\n\n\n\n\n\nAs you can see both the sum (total UMI per barcode) and the detected (number of transcripts with at least one UMI count) columns that were calculated by addPerCellQC() show that a very large portion of barcodes has only one read, then two, then three, etc. Due to the log-transformation in the plot scale, cells with sum = 0 were not plotted. We can still have an idea of their distribution:\n\nhead(table(sce$sum))\n\n\n    0     1     2     3     4     5 \n 7908 38805 17737 11049  5937  2856 \n\n\nWe also want to check that the two variables follow the expected relationship:\n\nplotColData(object = sce, y = \"sum\", x = \"detected\")\n\n\n\n\nExpectedly, as total counts increase more genes are detected.\n\n\n4.2.2 Barcode ranks - knee plots\nIt is difficult to establish what constitutes an empty droplet vs a “full” one just based on those plots. Ideally we would know where to establish a cutoff below which we consider each droplet to be empty.\nTo understand better where this cutoff may lie, we can plot the total UMI for each barcode (y axis) vs their rank (x axis), both log-transformed. This will give us a “knee plot” which should allow us to understand whether there are two more or less discrete populations (empty and full) and where the threshold may lie.\nWe take the code from the DropletUtils vignette.\n\nbr.out <- barcodeRanks(sce)\n\n# Making a plot.\nplot(br.out$rank, br.out$total, log=\"xy\", xlab=\"Rank\", ylab=\"Total\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): 7908 y values <= 0 omitted from\nlogarithmic plot\n\no <- order(br.out$rank)\nlines(br.out$rank[o], br.out$fitted[o], col=\"red\")\n\nabline(h=metadata(br.out)$knee, col=\"dodgerblue\", lty=2)\nabline(h=metadata(br.out)$inflection, col=\"forestgreen\", lty=2)\nlegend(\"bottomleft\", lty=2, col=c(\"dodgerblue\", \"forestgreen\"), \n    legend=c(\"knee\", \"inflection\"))\n\n\n\nmetadata(br.out)$inflection\n\n[1] 186\n\n\nWe see that a good threshold may lie at about 186 total UMIs for this dataset."
  },
  {
    "objectID": "qc.html#removing-empty-droplets-and-ambient-rna",
    "href": "qc.html#removing-empty-droplets-and-ambient-rna",
    "title": "4  Quality Control",
    "section": "4.3 Removing empty droplets and ambient RNA",
    "text": "4.3 Removing empty droplets and ambient RNA\nHaving established a threshold, we can use it to call the emptyDrops() function from DropletUtils.\nBy specifying the threshold we are instructing the algorithm that whatever lies below 186 UMIs can be considered “ambient RNA”. Then, the algorithm compares the distribution of counts in each cell to the ambient distribution, and tests how significantly different it is by permutation testing. More details on the algorithm and testing regime can be found by typing ?emptyDrops.\n\nempty_droplets <- emptyDrops(sce, lower = 186)\nkeep_droplets <- empty_droplets$FDR <= 0.01\nncol(sce)\n\n[1] 92846\n\nsum(keep_droplets, na.rm=TRUE)\n\n[1] 2802\n\n\nWe end up with 2801 “good quality” cells according to the content alone. This is a huge reduction considering we started with 92846 barcodes, and it means a lot of sequencing was inevitably wasted.\nWe can add this information to the colData of our object and plot the sum of UMI counts divided by ambient and non-ambient:\n\nsce$empty <- factor(ifelse(empty_droplets$FDR <= 0.01, \"ok\", \"empty\"))\nsce$empty[is.na(sce$empty)] = \"empty\"\nplotColData(object = sce, y = \"sum\", x = \"empty\", colour_by = \"empty\")\n\n\n\n\nWe also want to check that the number of permutations does not limit the ability of the algorithm to identify empty droplets:\n\ntable(Sig = keep_droplets, Limited = empty_droplets$Limited)\n\n       Limited\nSig     FALSE TRUE\n  FALSE   969    0\n  TRUE   2693  109\n\n\nSince there are no barcodes that have Limited == TRUE and Sig == FALSE we can safely assume the iterations were sufficient, and the result is reliable.\nWe can remove all the empty barcodes:\n\nsce = sce[, which(sce$empty == \"ok\"), drop = FALSE]"
  },
  {
    "objectID": "qc.html#as-you-will-remember-it-is-an-unfiltered-dataset-coming-from-a-single-mouse-sample.",
    "href": "qc.html#as-you-will-remember-it-is-an-unfiltered-dataset-coming-from-a-single-mouse-sample.",
    "title": "4  Quality Control",
    "section": "4.2 As you will remember, it is an unfiltered dataset coming from a single mouse sample.",
    "text": "4.2 As you will remember, it is an unfiltered dataset coming from a single mouse sample.\n\n4.2.1 Total counts and detected genes\nWe start by applying per-cell quality control using the scater package:\n\nlibrary(scuttle)\nlibrary(scater)\n\nsce = addPerCellQC(sce)\n\nWe can then visualize the metrics using violin plots:\n\nplotColData(object = sce, y = \"sum\") + scale_y_log10()\n\nWarning: Transformation introduced infinite values in continuous y-axis\nTransformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 7908 rows containing non-finite values (stat_ydensity).\n\n\nWarning: Removed 7908 rows containing missing values (geom_point).\n\n\n\n\nplotColData(object = sce, y = \"detected\") + scale_y_log10()\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 7908 rows containing non-finite values (stat_ydensity).\n\n\nWarning: Removed 7908 rows containing missing values (geom_point).\n\n\n\n\n\nAs you can see both the sum (total UMI per barcode) and the detected (number of transcripts with at least one UMI count) columns that were calculated by addPerCellQC show that a very large portion of barcodes has only one read, then two, then three, etc. Due to the log-transformation in the plot scale, cells with sum = 0 were not plotted. We can still have an idea of their distribution:\n\nhead(table(sce$sum))\n\n\n    0     1     2     3     4     5 \n 7908 38805 17737 11049  5937  2856 \n\n\nWe also want to check that the two variables follow the expected relationship:\n\nplotColData(object = sce, y = \"sum\", x = \"detected\")\n\n\n\n\nExpectedly, as total counts increase more genes are detected.\n\n\n4.2.2 Barcode ranks\nIt is difficult to establish what constitutes an empty droplet vs a “full” one just based on those plots. Ideally we would know where to establish a cutoff below which we consider each droplet to be empty.\nTo understand better where this cutoff may lie, we can plot the total UMI for each barcode (y axis) vs their rank (x axis), both log-transformed. This will give us a “knee plot” which should allow us to understand whether there are two more or less discrete populations (empty and full) and where the threshold may lie.\nWe take the code from the dropletUtils vignette.\n\nbr.out <- barcodeRanks(sce)\n\n# Making a plot.\nplot(br.out$rank, br.out$total, log=\"xy\", xlab=\"Rank\", ylab=\"Total\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): 7908 y values <= 0 omitted from\nlogarithmic plot\n\no <- order(br.out$rank)\nlines(br.out$rank[o], br.out$fitted[o], col=\"red\")\n\nabline(h=metadata(br.out)$knee, col=\"dodgerblue\", lty=2)\nabline(h=metadata(br.out)$inflection, col=\"forestgreen\", lty=2)\nlegend(\"bottomleft\", lty=2, col=c(\"dodgerblue\", \"forestgreen\"), \n    legend=c(\"knee\", \"inflection\"))\n\n\n\nmetadata(br.out)$inflection\n\n[1] 186\n\n\nWe see that a good threshold may lie at about 186 total UMIs for this dataset."
  },
  {
    "objectID": "qc.html#mitochondrial-content",
    "href": "qc.html#mitochondrial-content",
    "title": "4  Quality Control",
    "section": "4.4 Mitochondrial content",
    "text": "4.4 Mitochondrial content\nTo calculate mitochondrial content we isolate mitochondrial genes - conveniently labelled in their gene symbol by starting with “mt-” - and calculate the relative expression resulting in the percentage of mitochondrial content.\n\nmito_genes = grep(\"^mt-\", rowData(sce)$Symbol)\n\nsce = addPerCellQCMetrics(sce, subsets = list(\"mito\" = mito_genes))\n\nplotColData(object = sce, y = \"subsets_mito_percent\")"
  },
  {
    "objectID": "qc.html#distribution-based-thresholding",
    "href": "qc.html#distribution-based-thresholding",
    "title": "4  Quality Control",
    "section": "4.5 Distribution-based thresholding",
    "text": "4.5 Distribution-based thresholding\nWe can apply arbitrary thresholds, e.g. only retain cells with at least 400 total UMI counts, or below 5% mitochondrial content, but a more sophisticated approach consists in determining thresholds from the distributions themselves.\n\nWe use the isOutlier() function from scuttle to determine which barcodes are at least 3 MADs (Median Absolute Deviations) away from the median:\n\nlow_lib <- isOutlier(log10(sce$sum), type = \"lower\", nmad=3)\n\nlow_genes <- isOutlier(log10(sce$detected), type = \"lower\", nmad=3)\n\nhigh_mt <- isOutlier(sce$subsets_mito_percent, type = \"higher\", nmad = 4)\n\n\ndata.frame(LowLib=sum(low_lib), \n           LowNgenes=sum(low_genes), \n           HighMT = sum(high_mt))\n\n  LowLib LowNgenes HighMT\n1     25        16     53\n\nsce$discard <- low_lib | low_genes | high_mt \n\ntable(sce$discard)\n\n\nFALSE  TRUE \n 2738    64 \n\n\nWe don’t need to remove many cells at this stage. We highlight cells marked for elimination:\n\nplotColData(object = sce, y = \"sum\", colour_by = \"discard\")\n\n\n\n\nWe discard the marked barcodes:\n\nsce <- sce[, !sce$discard]"
  },
  {
    "objectID": "qc.html#doublet-identification",
    "href": "qc.html#doublet-identification",
    "title": "4  Quality Control",
    "section": "4.6 Doublet identification",
    "text": "4.6 Doublet identification\nWe use the scDblFinder package to identify doublets in the data.\nThis package simulates artificial doublets by over-clustering cells, then mixing them together across clusters by summing the reads, and using these artificial doublets to create a classifier (based on the XGBoost libraries) that is able to assign a doublet probability to every cell, and their most likely origin.\nOne of the parameters of scDblFinder is the expected number of doublets, which depends on the technology and the initial amount of cells profiled. You can refer to this link for 10X Genomics Chromium platforms and library preparation protocols.\n\n\n\n\n\n\nNote\n\n\n\nscDblFinder and other doublet finding methods work well to identify heterotypic doublets, i.e. doublets in which cells come from different populations/types. If two cells of the same or similar type are included in a doublet - termed homotypic - they will not be called. However, homotypic doublets are accounted for since even though they cannot be distinguished they will still be generated.\n\n\n\nlibrary(scDblFinder)\nsce <- scDblFinder(sce, clusters = TRUE)\n\nClustering cells...\n\n\n8 clusters\n\n\nCreating ~5000 artificial doublets...\n\n\nDimensional reduction\n\n\nEvaluating kNN...\n\n\nTraining model...\n\n\niter=0, 158 cells excluded from training.\n\n\niter=1, 129 cells excluded from training.\n\n\niter=2, 119 cells excluded from training.\n\n\nThreshold found:0.528\n\n\n133 (4.9%) doublets called\n\n\nPlotting the results:\n\nplotColData(object = sce, y = \"sum\", x =  \"scDblFinder.class\", colour_by = \"scDblFinder.class\")\n\n\n\n\nAs expected, doublets tend to have a higher total UMI content.\n\n\n\n\n\n\nImportant\n\n\n\nUnlike empty droplets, we do not remove doublets at this stage. We flag them and see where they end up in downstream applications such as dimensional reduction and clustering."
  },
  {
    "objectID": "qc.html#batch-aware-qc",
    "href": "qc.html#batch-aware-qc",
    "title": "4  Quality Control",
    "section": "4.7 Batch-aware QC",
    "text": "4.7 Batch-aware QC\nAll the operations outlined above work well for a dataset derived from a single “batch”, i.e. a single patient, or capture, animal, etc. However, when dealing with multiple samples, it is important to take systematic biases into account.\n\n\n\n\n\n\nNote\n\n\n\nWhat is a batch effect?\n“Whatever difference you do not care for but shows up nevertheless” would be a curt but still relevant answer.\nTo be more precise while still being brief, whenever we sequence samples coming from different sources - patients, animals, labs, platforms, days of the week, machines - we introduce a systematic bias, i.e. a “batch effect”, that needs to be corrected and/or accounted for. If we don’t, we will find differences that are due to unintersting variability (e.g. between patients, or between days of the week) which has no biological interpretation.\n\n\nTo illustrate these differences, let’s download a small multi-sample single-cell RNA-seq dataset from Muraro et al. 2016, using the scRNAseq package. This dataset contains pancreas cells from 4 donors, sorted in 8 plates each, and sequenced using the CEL-Seq2 technology, a full-length lower throughput platform.\n\nlibrary(scRNAseq)\nmuraro <- MuraroPancreasData()\n\nsnapshotDate(): 2022-04-26\n\n\nsee ?scRNAseq and browseVignettes('scRNAseq') for documentation\n\n\nloading from cache\n\n\nsee ?scRNAseq and browseVignettes('scRNAseq') for documentation\n\n\nloading from cache\n\n\nWe can see how cells from each donor have been sequenced in 8 plates:\n\ntable(muraro$donor, muraro$plate)\n\n     \n       1  2  3  4  5  6  7  8\n  D28 96 96 96 96 96 96 96 96\n  D29 96 96 96 96 96 96 96 96\n  D30 96 96 96 96 96 96 96 96\n  D31 96 96 96 96 96 96 96 96\n\n\nIs there any systematic bias between plates and/or donors in the distribution of counts?\n\nmito_genes = grep(\"^MT-\", rowData(muraro)$symbol)\n\nmuraro = addPerCellQCMetrics(muraro, subsets = list(\"mito\" = mito_genes))\n\nplotColData(object = muraro, x = \"donor\", y = \"sum\", colour_by = \"donor\") + \n  scale_y_log10() +\n  ggtitle(\"Total UMI content by donor\")\n\n\n\nplotColData(object = muraro, x = \"donor\", y = \"subsets_mito_percent\", colour_by = \"donor\") + \n  ggtitle(\"Mitochondrial content by donor\")\n\n\n\nplotColData(object = muraro, x = \"plate\", y = \"sum\", colour_by = \"plate\") + \n  scale_y_log10() +\n  ggtitle(\"Total UMI content by plate\")\n\n\n\nplotColData(object = muraro, x = \"plate\", y = \"subsets_mito_percent\", colour_by = \"plate\") + \n  ggtitle(\"Mitochondrial content by plate\")\n\n\n\n\nFirst things first: this dataset has no mitochondrial transcript quantification, nor does it require removal of empty droplets. Moreover, CEL-seq2 has an extremely low expected doublet rate (< 1%, see Ding et al. Nature Biotechnology 2020).\nYou can see that, while plates are mostly similar, donors have different distributions of total read count. This means that applying a cutoff on the whole dataset without dividing it by donor will both include and remove several cells that, in a single datasets, would have been excluded/included.\nThe batch argument in isOutlier() allows to take this effect into account by applying the distribution-based cut-offs separately in each batch. In this case the batch argument is a vector containing the donor information, i.e. muraro$donor. In other cases it may just be sufficient to specify the name (character) of the column in colData for the function to know where to retrieve the batch labels.\n\nlow_lib <- isOutlier(log10(muraro$sum), type = \"lower\", nmad=3, batch = muraro$donor)\n\nlow_genes <- isOutlier(log10(muraro$detected), type = \"lower\", nmad=3, batch = muraro$donor)\n\n\n\ndata.frame(LowLib=sum(low_lib), \n           LowNgenes=sum(low_genes))\n\n  LowLib LowNgenes\n1    250       285\n\nmuraro$discard <- low_lib | low_genes\n\ntable(muraro$discard)\n\n\nFALSE  TRUE \n 2787   285 \n\nplotColData(object = muraro, x = \"donor\", y = \"sum\", colour_by = \"discard\") + \n  scale_y_log10() +\n  ggtitle(\"Total UMI content by donor\")\n\n\n\n\nSimilarly, other functions (such as scDblFinder) require you to specify the batch to make correct inferences about doublets."
  },
  {
    "objectID": "normalization.html",
    "href": "normalization.html",
    "title": "5  Normalization and variance stabilization",
    "section": "",
    "text": "Relevant software:\n\n\n\nscuttle (R Bioconductor)\nsctransform (R CRAN)\nMatrixGenerics (R Bioconductor)"
  },
  {
    "objectID": "normalization.html#depth-normalization",
    "href": "normalization.html#depth-normalization",
    "title": "5  Normalization and variance stabilization",
    "section": "5.1 Depth normalization",
    "text": "5.1 Depth normalization\nIn the QC chapter we have seen a large variability in terms of total transcript/UMI count per cell. This is caused by both biological aspects - some cells make more and/or longer transcripts than others - and by technical aspects - transcripts from some cells have been captured or sequenced with a higher efficiency on average. Put it simply, it means that two very similar cells (same cell type/state, same phase of the cell cycle, same everything) may still look different just because 3000 UMI/reads total were counted for one and 6000 for the other. We say that these two cells are sequenced at a different depth.\nThis means that the naive comparison of cells can be distorted by this difference in depth: if we were to order cells by the variability of their genes without taking these differences into account, we would order them according to their respective depth and not any other biologically relevant feature.\nNormalization is a process that transforms the counts in a way that accounts for these differences, effectively rescaling them so that cells become more comparable. The per-cell scaling constant that is applied to equalize depths is commonly called the size factor.\nOne of the easiest ways to normalize cells is to divide each cell’s counts by a size factor equal to the sum total of counts (depth), and then multiply by a constant (10K or 1M).\nTaking a raw count matrix \\(X\\) with \\(m\\) genes and \\(n\\) cells, and indexing by \\(i = \\{1, 2, 3, ... ,m\\}\\) and \\(j = \\{1, 2, 3, …, n\\}\\) , the depth-normalized matrix \\(X'\\) is given by:\n\\[\nX'_{i,j} = \\frac{X_{i,j}}{\\sum_{i=1}^{m}x_{i,j}}\\cdot10^{6}\n\\]\nThis is the depth-normalization as implemented in Seurat.\nDifferent versions of depth normalization are implemented by scuttle in the SingleCellExperiment toolset. There are two main options:\n\ncomputeLibraryFctors(): naive depth normalization where library sizes are scaled so that across cells their mean is equal to 1 (it is also possible to use the median or geometric mean). This is achieved by simply dividing the library sizes by their mean, then dividing the cells by these size factors. The assumptions behind this method are that there is no imbalanced differential expression between cells, i.e. up-regulation of a set of genes in one cell is matched by down-regulation of another set of cells in any other cell, so that there is no “excess differential expression”. This is the assumption that is used in bulk RNA-seq data analysis, but it is less likely to hold in single-cell data, especially when dealing with non-homogeneous datasets with different cell types and compositions and because of the prevalence of zero counts. This can also be referred to as “proportional fitting” (see below).\ncomputeSumFactors(): normalization by pooling and deconvolution: size factors are estimated by creating a reference “pseudocell” (averaging across all cells), pooling samll groups of cells together aggregating their counts by sum, and dividing the pooled counts by the pseudocell library size. This procedure is repeated iteratively including different cells in each iteration so that, eventually, a linear system of equations can be constructed which allows to derive the per-cell size factor. The advantages over the more naive size factor estimation are twofold: 1) the presence of 0 counts is alleviated by aggregating across cells, and 2) the compositional effects are taken into account by pre-clustering cells and using only cells within a cluster as pseudocell reference. This procedure is computationally more intensive."
  },
  {
    "objectID": "normalization.html#variance-stabilization",
    "href": "normalization.html#variance-stabilization",
    "title": "5  Normalization and variance stabilization",
    "section": "5.2 Variance stabilization",
    "text": "5.2 Variance stabilization\nAs important as it is, depth normalization is not enough to understand the “biological structure” of the data, i.e. to remove the technical confounding effects.\nCounts from an RNA-seq experiment normally follow a Negative Binomial or a Poisson distribution depending on the capture and sequencing technology. These distributions arise due to several factors, both intrinsically technical (capture and reverse transcription efficiency in the presence of competition from highly expressed transcripts) and biological (bursty transcription and RNA degradation).\nA notable feature of these distribution is that they exhibit a positive mean-variance relationship: as the mean expression of a gene increases, so does its variance.\nThis relationship holds even if we apply depth normalization.\nLet’s simulate some data to convince ourselves, borrowing from examples in the scran package.\nIf we simulate counts from a Poisson distribution, we will have - in the un-normalized case - a linear trend where mean = variance:\n\nlibrary(MatrixGenerics)\n\nLoading required package: matrixStats\n\n\n\nAttaching package: 'MatrixGenerics'\n\n\nThe following objects are masked from 'package:matrixStats':\n\n    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,\n    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,\n    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,\n    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,\n    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,\n    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,\n    colWeightedMeans, colWeightedMedians, colWeightedSds,\n    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,\n    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,\n    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,\n    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,\n    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,\n    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,\n    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,\n    rowWeightedSds, rowWeightedVars\n\n# simulate Poisson distributed counts for 200 cells, 2000 genes with no structure\n\nset.seed(100)\nncells <- 200\nngenes <- 2000\n\n# Simulate gene means: 2 to the power of a uniformly random number (min 2, max 10)\n# they will be the lambda parameter for the Poisson distribution\ngene_means <- 2^runif(ngenes, 2, 10)\n\n# Simulate counts\nsim <- matrix(rpois(ngenes*ncells, lambda = gene_means), ncol = ncells)\n\n# Plot mean and variance\nplot(x = rowMeans(sim), y = rowVars(sim), xlab = \"mean\", ylab = \"variance\")\nabline(0, 1, col = \"red\")\n\n\n\n\nAfter correction, the trend is still linear, albeit no longer equal (these normalized counts are no longer Poisson distributed):\n\n# Seurat epth normalization\nsim_norm = t(t(sim)/colSums(sim))*1e6\n\n# Plot mean and variance after normalization\nplot(x = rowMeans(sim_norm), y = rowVars(sim_norm), xlab = \"mean\", ylab = \"variance\")\ntrend = lm(rowVars(sim_norm) ~ rowMeans(sim_norm))\nabline(trend, col = \"red\")\nabline(0, 1, col = \"blue\")\n\n\n\n\n\n# scuttle depth normalization\nsim_norm = t(t(sim)/(colSums(sim)/mean(colSums(sim))))\n\n# Plot mean and variance after normalization\nplot(x = rowMeans(sim_norm), y = rowVars(sim_norm), xlab = \"mean\", ylab = \"variance\")\nabline(0,1, col = \"red\")\n\n\n\n\nThe NB distribution can be characterized by two parameters: \\(\\mu\\) and \\(\\phi\\), i.e. mean and dispersion (mu and size in the rnbinom() function).\nThe dispersion parameter models the fact that in a NB distribution, compared to a Poisson, the variance increases at a higher rate than the mean; in other words the data are “overdispersed”. This dispersion parameter in our simulation is gene-specific and accounts for the aforementioned intrinsic technical variability.\n\n# simulate Negative Binomial distributed counts for 200 cells, 2000 genes with no structure\n\nset.seed(100)\n\nncells <- 200\nngenes <- 2000\n\n# Simulate gene means: 2 to the power of a uniformly random number (min 2, max 10)\n# they will be the mu parameter for the NB distribution\ngene_means <- 2^runif(ngenes, 2, 10)\n\n# Simulate dispersions (size parameter)\ndispersions <- 10/gene_means + 0.2\n\n# Simulate counts\nsim_nb <- matrix(rnbinom(ngenes*ncells, mu = gene_means, size = 1/dispersions), ncol = ncells)\n\n# Plot mean and variance\nplot(x = rowMeans(sim_nb), y = rowVars(sim_nb), xlab = \"mean\", ylab = \"variance\")\n\n\n\n\nUpon depth normalization:\n\n# Seurat depth normalization\nsim_nb_norm_seurat = t(t(sim_nb)/colSums(sim_nb))*1e6\n\n# Plot mean and variance after normalization\nplot(x = rowMeans(sim_nb_norm_seurat), y = rowVars(sim_nb_norm_seurat), \n     xlab = \"mean\", ylab = \"variance\")\n\n\n\n\n\n# scuttle depth normalization\nsim_nb_norm_scuttle = t(t(sim_nb)/(colSums(sim_nb)/mean(colSums(sim_nb))))\n\n# Plot mean and variance after normalization\nplot(x = rowMeans(sim_nb_norm_scuttle), y = rowVars(sim_nb_norm_scuttle), \n     xlab = \"mean\", ylab = \"variance\")\n\n\n\n\nAs you can see, removing the depth bias does not take care of removing the trend between variance and mean.\nRecall that we need to order genes by their variances because we are interested in using the genes that exhibit the largest (biological) variability, as a way to filter out noise - genes that don’t change along biological axes of variation - and to reduce computational complexity by reducing the initial dimensions e.g. from 30,000 to 2,000.\nIf we were to select genes only based on their variance, ignoring the mean-variance relationship means we will choose highly expressed genes; among these there will be several housekeeping genes whose expression is unlikely to reveal the biological variability that is useful to separate cells in clusters or order them along differentiation processes.\nVariance stabilization is a procedure that tries to uncouple the relationship between variance and mean, so that the selection of highly variable genes does not necessarily include only the most expressed ones. Variance stabilization per se does not account for different depth, so it is usually applied after depth normalization. However, the sctransform procedure operates directly on raw counts, as it includes different depths in its modelling step.\n\n5.2.1 Variance stabilization procedures\nThere are several methods for variance stabilization, and we can briefly explore/simulate some of them in conjunction with depth normalization:\n\nlog1p: this is a simple log2 transformation, where a “pseudocount” of 1 is added to each count before applying the logarithm to account for 0’s.\n\nsim_nb_log1p = log2(sim_nb + 1)\nplot(x = rowMeans(sim_nb_log1p), y = rowVars(sim_nb_log1p), \n     xlab = \"mean\", ylab = \"variance\",\n     main = \"No depth normalization\")\n\n\n\nsim_nb_norm_scuttle_log1p = log2(sim_nb_norm_scuttle + 1)\nplot(x = rowMeans(sim_nb_norm_scuttle_log1p), y = rowVars(sim_nb_norm_scuttle_log1p), \n     xlab = \"mean\", ylab = \"variance\",\n     main = \"Scuttle depth normalization\")\n\n\n\nsim_nb_norm_seurat_log1p = log2(sim_nb_norm_seurat + 1)\nplot(x = rowMeans(sim_nb_norm_seurat_log1p), y = rowVars(sim_nb_norm_seurat_log1p), \n     xlab = \"mean\", ylab = \"variance\",\n     main = \"Seurat depth normalization\")\n\n\n\n\nsquare root: variance is stabilized by taking the square root of counts. This does not require adding a pseudocount.\n\n# Poisson sqrt variance stabilization\nsim_sqrt = sqrt(sim)\nplot(x = rowMeans(sim_sqrt), y = rowVars(sim_sqrt), xlab = \"mean\", ylab = \"variance\")\n\n\n\n# NB sqrt variance stabilization\nsim_nb_sqrt = sqrt(sim_nb)\nplot(x = rowMeans(sim_nb_sqrt), y = rowVars(sim_nb_sqrt), xlab = \"mean\", ylab = \"variance\")\n\n\n\n\n\nThe sqrt transformation is mostly successful with Poisson distributed counts, but not with NB distributed counts.\n\nPFlog1pPF: this is a term introduced by Booeshaghi et al. (2022), which consists in one round of depth normalization, followed by log1p transformation, and an additional round of depth normalization (or “proportional fitting” as they call it).\n\n\nsim_nb_norm_scuttle_log1p = log2(sim_nb_norm_scuttle + 1)\nsf = colSums(sim_nb_norm_scuttle_log1p)/mean(colSums(sim_nb_norm_scuttle_log1p))\nsim_nb_norm_log1p_pf = t(t(sim_nb_norm_scuttle_log1p)/sf)\n                           \nplot(x = rowMeans(sim_nb_norm_log1p_pf), y = rowVars(sim_nb_norm_log1p_pf), \n     xlab = \"mean\", ylab = \"variance\",\n     main = \"PFlog1pPF\")\n\n\n\n\n\nsctransform and the function vst(). This is a more complex procedure that implements both normalization and variance stabilization, together with highly variable gene selection (see later). Briefly, a regularized negative binomial regression model is applied to each gene, which allows the introduction of covariates (such as batch labels, sex, age, etc). Then the Pearson residuals (residuals divided by the square root of the expected value) for each gene are taken as “corrected” (normalized and stabilized) gene expression values. The variance of Pearson residuals can be used to rank genes as “highly variable”.\n\n\n\nCalculating cell attributes from input UMI matrix: log_umi\n\n\nVariance stabilizing transformation of count matrix of size 2000 by 200\n\n\nModel formula is y ~ log_umi\n\n\nGet Negative Binomial regression parameters per gene\n\n\nUsing 2000 genes, 200 cells\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================================| 100%\n\n\nSecond step: Get residuals using fitted parameters for 2000 genes\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCalculating gene attributes\n\n\nWall clock passed: Time difference of 1.619893 secs\n\n\n\nplot(rowMeans(sim_sct$y), rowVars(sim_sct$y))"
  },
  {
    "objectID": "datainput.html#what-does-the-data-look-like",
    "href": "datainput.html#what-does-the-data-look-like",
    "title": "3  Data input",
    "section": "3.7 What does the data look like?",
    "text": "3.7 What does the data look like?\nNow is a good time to try to understand what the count data from a single cell transcriptomics experiment looks like, and introduce two important concepts.\nThe count table is a matrix (or 2D array) that contains m genes (usually rows) and n cells (columns). Every cell of this matrix will contain a positive integer value - the count for a particular gene in a particular cell. These matrices will have two intrinsic important features:\n\nsparsity: because of the high throughput of these measurements (both in terms of numbers of cells and numbers of genes), the low absolute amount of input to be amplified during the library preparation, and the stochasticity of transcription, most of the genes will not be reliably quantified. In other words, many counts will be 0.\n\nObviously, a gene that is not expressed will not be quantified. However, a gene that is expressed at a low or even mid level, and that would normally be measured by a bulk RNA-seq experiment, will be bypassed in a single-cell RNA-seq experiment. This is historically referred to as the “dropout” problem, because genes that would be normally detectable “drop out” of the measurement due to technical (or biological) reasons.\n\nSparse data presents many challenges: does an average gene expression value make sense in the presence of a majority of 0 counts? Are the counts zero-inflated, or do they follow distributions more traditionally associated with RNA-seq? How noisy does the data get in the presence of a majority of 0’s?\n\nSeveral tools have been developed to deal with sparse data, and the single cell analysis pipelines make use of them to overcome its limitations. However, an important maxim to keep in mind when dealing with this data is that the absence of evidence is not the evidence of absence. A gene may not be measured even if it is expressed, and its activity may need to be inferred in many cases.\nhigh dimensionality: this is again a consequence of the high throughput, especially in terms of number of genes (features) measured. To better explain this concept, imagine you analyzed 100 cells and for each cell only measured 2 genes. You can plot these cells in a 2D space, positioning cells according to the conbined expression of these 2 genes. Greatly simplifying, you could divide cells in 4 possible “areas”: gene 1 high and gene 2 low, gene 2 high and gene 1 high, both high, both low.\n\nNow imagine adding a third gene. The 2D space becomes a 3D space, and the number of combinations increases by a factor of 2, resulting in 8 combinations (HHH, LLL, HLL, HLH, LLH, LHL, LHH, HHL). As we add genes - remember we are measuring several thousands of them! - the number of possible combination a single cell can inhabit increases.\n\nSome issues start arising: in the Summary I briefly mentioned that we need to relate cells to each other in terms of their distance in space. If these genes determine a coordinate system, then it is mathematically possible to calculate the Euclidean distance between two cells \\(p\\) and \\(q\\) using the Pythagorean theorem in \\(n\\) dimensions (\\(n\\) genes):\n\n\\[\nd(p, q) = \\sqrt{(p_{1} - q_{1})^{2} + (p_{2} - q_{2})^{2} + (p_{3} - q_{3})^{2} + ... + (p_{n} - q_{n})^{2}}\n\\]\n\nHowever, as n increases, all points (cells) tend to become equidistant. This is known as the curse of dimensionality and it can be understood more in depth in this Appendix. For now, it is sufficient to know that dealing with distances in high dimensional data “as is” is unlikely to yield sensible results, so that we need to find ways to reduce the dimensions.\n\nWe refer to the m x n matrix as the count matrix, or raw count matrix, as it is untransformed and comes directly from the output of pre-processing pipelines. Some operations need to be performed using raw counts, whereas others require normalized or otherwise transformed counts. It is very important to understand when we need to use one or another as the statistical properties of counts (positive integers) are very different from those of transformed values that can take fractional and/or non-positive values."
  },
  {
    "objectID": "appendices.html",
    "href": "appendices.html",
    "title": "6  Appendices",
    "section": "",
    "text": "As mentioned in the Data Input chapter, single cell count matrices are highly dimensional, and this represents a problem, in that Euclidean distances between points in high dimensional space tend to become equivalent.\nTo have a geometric intuition of why this may be the case, let’s consider a construction where we inscribe a circle inside a square. The radius of the circle is \\(r\\), meaning that the side of the square is \\(2r\\). The area of the circle is \\(\\pi r^2\\) and the area of the square is \\(4r^2\\). The ratio between the two areas is then \\(\\frac{\\pi r^2}{4 r^2} = \\frac{\\pi}{4}\\), roughly equivalent to 78.53%\nNow we add a dimension: a sphere included in a cube. The volume of the sphere is \\(\\frac{4}{3}\\pi r^3\\), and the volume of the cube is \\(8r^3\\). Then, the ratio between the volumes is \\(\\frac{\\frac{4}{3}\\pi r^3}{8r^3}= \\frac{\\pi}{6}\\) which is roughly equivalent to 52.36%.\nWe can keep adding dimensions considering that the volume of a d-hypersphere (a sphere in d-dimensions) of radius r is given by the following formula:\n\\[\nV_{d}(r)={\\frac {\\pi ^{d/2}}{\\Gamma {\\bigl (}{\\tfrac {d}{2}}+1{\\bigr )}}}r^{d}\n\\]\nLet’s simulate it in R, using a radius r = 0.5 so that the side of the cube is = 1:\n\ncube = function(r, d){\n  r ** d\n}\n\nhypersphere = function(r, d){\n  (pi ** (d / 2) / (gamma((d/2) + 1))) * (r ** d)\n}\n\ndimensions = 2:100\nradius = 0.5\n\ncube_volumes = cube(radius*2, dimensions)\nhypersphere_volumes = hypersphere(radius, dimensions)\n\nratios = hypersphere_volumes/cube_volumes\nplot(x = dimensions, y = ratios, pch = 16, cex = 0.5)\nabline(h = 0, lty = 2)\n\n\n\n\nAs we increase the dimension, the ratio will keep decreasing - effectively the relative volume of the sphere approaches zero as the dimensionality approaches infinity. The volume of the cube, however, always has a volume of 1 since its side is = 1.\nHow does this relate to the distance, though?\n{TO BE CONTINUED}"
  }
]